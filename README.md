# K-Nearest Neighbor 

In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:
-> In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.
-> In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.

## Description

In this code KNN is used to predict different classes for a given image. The dataset used are USPS and MNIST.

## Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

